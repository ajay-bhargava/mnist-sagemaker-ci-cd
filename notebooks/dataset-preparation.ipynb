{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "Here we will use the `fetch20_newsgroups` dataset from `sklearn.datasets` to create a dataset for our model. We will use the `train_test_split` function from `sklearn.model_selection` to split the dataset into training and testing sets. \n",
    "\n",
    "These data will then be uploaded to S3 for use in a BERT Topic Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define `S3`\n",
    "Here we will define the `S3` bucket and prefix where we will upload our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    \"\"\"Create an S3 bucket in a specified region.\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None or region == 'us-east-1':\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_in_bucket(bucket_name: str, folder_name: str) -> None:\n",
    "    \"\"\"Creates a folder in the specified S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket.\n",
    "        folder_name (str): The name of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    if not folder_name.endswith('/'):\n",
    "        folder_name += \"/\"\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket(bucket_name='with-context-sagemaker', region = 'us-east-1')\n",
    "create_folder_in_bucket(bucket_name='with-context-sagemaker', folder_name ='/datasets/bert-topic/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import the dataset\n",
    "In this step we perform the EDA and data engineering to ETL that data into a format that can be used by our model in Sagemaker. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "data = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "\n",
    "documents = data['data']\n",
    "documents = documents[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're limiting to 100 documents for ease of training.\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2A: Prepare the data\n",
    "Not performed here, but, this is where you'd likely be doing some form of data preparation if training for the first time in Sagemaker. This can be automated the second and third time around. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Write Dataset\n",
    "We will use the `tempfile` library to write the data to a temporary file. This file will then be uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "\n",
    "def upload_to_s3(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket.\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found\")\n",
    "        return False\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Create a temporary file and upload it to S3\n",
    "with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix = '.txt') as temp_file:\n",
    "    # Write the documents to the temporary file\n",
    "    for document in documents:\n",
    "        temp_file.write(\n",
    "            document.replace('\\n', '\\\\n') + '\\n'\n",
    "        )\n",
    "\n",
    "    # Get the path of the temporary file\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "    # Upload file to S3\n",
    "    upload_to_s3(\n",
    "        file_name = temp_file_path,\n",
    "        bucket = \"with-context-sagemaker\",\n",
    "        object_name = \"datasets/bert-topic/training_file.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create the Output S3 Directories\n",
    "Here we will create the output directories in S3 where the data will be uploaded to following Sagemaker Estimator training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_in_bucket(\n",
    "    bucket_name='with-context-sagemaker',\n",
    "    folder_name ='fits/bert-topic/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Pocket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist-sagemaker-ci-cd-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
